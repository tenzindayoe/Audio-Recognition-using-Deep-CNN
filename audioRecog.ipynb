{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydub\n",
    "from scipy.fft import fft\n",
    "import cv2\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2232920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validated_file = 'validation_list.txt'\n",
    "%matplotlib inline\n",
    "# Load the validated.tsv file into a pandas DataFrame\n",
    "df = pd.read_csv(validated_file, sep=\" \", header=None)\n",
    "\n",
    "labels = []\n",
    "feature_vectors = []\n",
    "\n",
    "# Loop through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    file_name = row[0]\n",
    "    file_count = index + 1\n",
    "    print(f'Processing file {file_count} of 10000: {file_name}')\n",
    "    \n",
    "    # Extract the path and sentence from the row\n",
    "    path = file_name\n",
    "    sentence = file_name.split(\"/\")[0]\n",
    "    \n",
    "    # Load the audio clip and extract the FFT coefficients\n",
    "    \n",
    "    sampFreq, sound = wavfile.read(path)\n",
    "    sound = sound / 2.0**15\n",
    "    \n",
    "    frequencies, times, spectrogram = signal.spectrogram(sound, fs=sampFreq, nperseg=1024, noverlap=512)\n",
    "\n",
    "    # Convert the spectrogram to a logarithmic scale\n",
    "    spectrogram = np.log(spectrogram + 1e-9)\n",
    "\n",
    "    # Resize the spectrogram to a fixed size, if necessary\n",
    "    spectrogram_resized = cv2.resize(spectrogram, (200, 200))\n",
    "\n",
    "    labels.append(sentence)\n",
    "    feature_vectors.append(spectrogram_resized)\n",
    "    \n",
    "# Stack the feature vectors into a single numpy array\n",
    "feature_matrix = np.stack(feature_vectors)\n",
    "\n",
    "# Save the labels and feature matrix to disk as numpy arrays\n",
    "np.save('labels_specto.npy', labels)\n",
    "np.save('features_specto.npy', feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processor \n",
    "\n",
    "\n",
    "# Load the features and labels\n",
    "features = np.load('features_specto.npy')\n",
    "labels = np.load('labels_specto.npy')\n",
    "\n",
    "\n",
    "test_features = np.load('features_specto_test.npy')\n",
    "test_labels  = np.load('labels_specto_test.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Take the first 6000 elements from the shuffled test data and add to the training data\n",
    "features = np.concatenate([features, test_features], axis=0)\n",
    "labels = np.concatenate([labels, test_labels], axis=0)\n",
    "features, labels = shuffle(features, labels, random_state=92)\n",
    "features, labels = shuffle(features, labels, random_state=42)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features = scaler.fit_transform(features.reshape(-1, 1)).reshape(features.shape)\n",
    "\n",
    "\n",
    "#Test feature and label pre processing \n",
    "\n",
    "\n",
    "# Change the data type to float64\n",
    "features = features.astype('float64')\n",
    "\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Convert labels to integers\n",
    "labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "features = features.reshape(-1, 200,200, 1)\n",
    "test_features = test_features.reshape(-1,200,200,1)\n",
    "test_features =features[15000:]\n",
    "test_labels = labels[15000:]\n",
    "features  = features[0:15000]\n",
    "labels = labels[0:15000]\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator object and specify the desired transformations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "# Fit the ImageDataGenerator to the training data\n",
    "datagen.fit(features)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a generator for the training data using the ImageDataGenerator\n",
    "train_generator = datagen.flow(\n",
    "    features,\n",
    "    labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create a generator for the test data without data augmentation\n",
    "test_generator = ImageDataGenerator().flow(\n",
    "    test_features,\n",
    "    test_labels,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c947605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Deep CNN Network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional block\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(200,200, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second convolutional block\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(features,labels, epochs=20, validation_split=0.25)\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_features, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "# print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e082dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.title(\"Custom Deep CNN Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet Implementation\n",
    "mod = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(200,200,1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "mod.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = mod.fit(features,labels, epochs=20, validation_split=0.15)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = mod.evaluate(test_features, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.title(\"Resnet Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d189c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
